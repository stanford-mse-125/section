{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbf2241-1dfc-4d6f-9de0-9949db4c0307",
   "metadata": {},
   "source": [
    "# Discussion 3: Introduction to inference with M&M's\n",
    "Josh Grossman"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d3d7eeb0-9bdf-4d29-8c3e-fcd0b69bd7d3",
   "metadata": {},
   "source": [
    "<img src=\"img/mm.jpg\" alt= \u201cm&ms\u201d width=\"100\" /> <img src=\"img/mm.jpg\" alt= \u201cm&ms\u201d width=\"100\" /> <img src=\"img/mm.jpg\" alt= \u201cm&ms\u201d width=\"100\" /> <img src=\"img/mm.jpg\" alt= \u201cm&ms\u201d width=\"100\" /> <img src=\"img/mm.jpg\" alt= \u201cm&ms\u201d width=\"100\" /> <img src=\"img/mm.jpg\" alt= \u201cm&ms\u201d width=\"100\" />"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a1259f7f-8c60-4c39-b911-70881745a916",
   "metadata": {},
   "source": [
    "Welcome to the third discussion of MS&E 125!\n",
    "\n",
    "This discussion will proceed a little differently than previous discussion sections. If you join the discussion live, we'll be completing an in-class activity to generate (and eat!) our own data. If you're joining aynchronously, you'll be able to use synthetic (and inedible \ud83d\ude41) data to complete the notebook."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "ddcf0f8d-c481-403e-adc1-f3c359abb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a78542-da8f-46cc-9df1-060b86b96573",
   "metadata": {},
   "source": [
    "## Introduction"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9c6804b1-a63e-471d-8ae1-8f9fab27ceca",
   "metadata": {},
   "source": [
    "Suppose M&M's bags are filled with candy by an unobserved machine. It might looks something like this:"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a25ed576-accb-4737-9251-0c5171ca57a1",
   "metadata": {},
   "source": [
    "<img src=\"img/factory.jpg\" alt= \u201cm&msfactory\u201d width=\"300\" />"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2d824447-9345-42fe-b0d5-16d1abf82613",
   "metadata": {},
   "source": [
    "As experienced candy-connoisseurs and budding data scientists, suppose we're interested in an important question:\n",
    "\n",
    "- On average, what proportion of M&M's are blue?\n",
    "\n",
    "In other words, we're interested in **inferring a property of an unobserved machine that randomly generates bags of M&Ms.** \n",
    "\n",
    "- If we designed the machine ourselves, then we wouldn't need the tools of statistical inference. We could just read off the setting for \"proportion of candies that are blue\".\n",
    "\n",
    "- Instead, all we observe is a single bag of M&M's. \n",
    "\n",
    "If you've seen the [The Wizard of Oz](https://www.youtube.com/watch?v=ivRKfwmgrHY), you might draw an analogy to the unobserved man behind the curtain controlling what Dorothy et al. observe:\n",
    "\n",
    "<img src=\"img/curtain.jpg\" alt= \u201cwizardofoz\u201d width=\"300\" />\n",
    "\n",
    "This discussion will lead you through the process of turning a single bag of M&M's (the *sample*) into a statistical inference of the properties of an unobserved M&M's machine (the *population*, or the *data-generating process*). \n",
    "\n",
    "> At this point, it's natural to worry that filling bags with candy is totally unrelated to your future career prospects. However, this setup is surprisingly common in business settings. \n",
    ">\n",
    "> For example, suppose you're a product manager who is interested in understanding your customer base. If we survey a random sample of customers, we can think of the opinions of the entire customer base as the properties unobserved M&M's machine, and the survey results as the observed bag of M&M's. \n",
    ">\n",
    "> Same idea if you're a pollster trying to understand the fraction of all voters who identify as Republicans when all you observe is a small sample of voters.\n",
    ">\n",
    "> The methods taught in this notebook are used *constantly* by practitioners."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "31cc05da-d7b6-4ef4-9d6e-ac5ab330050c",
   "metadata": {},
   "source": [
    "## \ud83c\udf6c Generating the data"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c12eeacc-ce4a-415a-b3d7-c619b9767d1b",
   "metadata": {},
   "source": [
    "Once your bag of M&M's is in hand, **don't eat any!** Eating comes later.\n",
    "\n",
    "**Before you eat any M&M's**, count the number of blue and not blue M&M's in your bag. \n",
    "\n",
    "- Use the code cell below to record your count.\n",
    "\n",
    "- Enter your counts in [this Google form](https://docs.google.com/forms/d/e/1FAIpQLSeNrmsL6TE71Q3XgkVa6LOBCBi3mlkFfyW6IQ_-myC7rEIQNA/viewform?usp=sf_link)\n",
    "\n",
    "- If you're joining asynchronously, set `n_blue = 8` and `n_not_blue = 48`."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "9ad694d5-eb90-439f-ae37-57c20ad3d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_blue = 0\n",
    "\n",
    "n_not_blue = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4696e1a-5a64-44fd-a8a5-d0e040dfb0a0",
   "metadata": {},
   "source": [
    "## \ud83c\udf7d\ufe0f Interlude: Eating M&M's"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "525e375e-a22e-4bea-89c7-63d2741bd1dd",
   "metadata": {},
   "source": [
    "After you have counted your M&M's, you can eat them.\n",
    "\n",
    "Congratulations on making it through a very tedious counting exercise!"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e7886e9e-f2ea-436d-8952-b173730233e1",
   "metadata": {},
   "source": [
    "## \u261d\ufe0f Point estimates"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "85cef4e2-08e2-4c7a-9f9c-e6ebdd35dd4a",
   "metadata": {},
   "source": [
    "Our first objective is to provide a *point estimate*, or single best guess, of the *population* proportion of M&M's that are blue.\n",
    "\n",
    "- This is a setting on the M&M's machine. Remember, **the settings are not observed by us!** If they were observed, we wouldn't need the tools of inference.\n",
    "\n",
    "In the code cell below, provide a point estimate for the population proportion of M&M's that are blue. "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "d7a2a4e4-6d4d-4c95-924e-5c2bb8b68db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the code below!\n",
    "\n",
    "est_pop_prop_blue = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a27f72-52bc-4610-b149-eed754e8dc81",
   "metadata": {},
   "source": [
    "## \u2753 Uncertainty"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "92927837-5311-409f-8955-668e2988db80",
   "metadata": {},
   "source": [
    "Point estimates are often straightforward. For example, a sample statistic is often a very good estimate of a population statistic. \n",
    "\n",
    "Here's the problem: With only one bag of M&M's, how sure are you of your point estimate? \n",
    "\n",
    "- If you were instead given a different bag of M&M's, would you have had the same point estimate?\n",
    "\n",
    "- If you were instead given a smaller bag of M&M's, would you be less confident of your point estimate? \n",
    "\n",
    "- If you were instead given a Costco-sized plastic tub of M&M's, would you be more confident of your point estimate? \n",
    "\n",
    "What's going on here is *counterfactual reasoning*. In statistical inference, we need to think about **what could have happened in a parallel universe**.\n",
    "\n",
    "The (unobserved) distribution of point estimates across these parallel universes is called a *sampling distribution*. This idea powers inferential statistics!"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "317a0ec2-7117-4e2c-980b-fc6820cca054",
   "metadata": {},
   "source": [
    "## \ud83d\udc69\u200d\ud83d\ude80 Observing parallel universes?!"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8074390c-03b8-4959-8b93-329bbc1d55b6",
   "metadata": {},
   "source": [
    "We're in an exciting scenario where we can actually *observe parallel universes* where other point estimates were generated. All of your neighbors have calculated their own point estimates!\n",
    "\n",
    "> It's important to stress that **this is an unrealistic scenario**. We normally only see one sample of data. \n",
    "\n",
    "If we plot the point estimates from our neighbors, we can get an approximation of the theoretical sampling distribution.\n",
    "\n",
    "**Exercise**: In the cell below, plot the distribution of your and your neighbor's point estimates for the proportion of M&M's that are blue."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "a571c0a7-a02f-4b91-80f0-114d1f3ed92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ffbb0-40af-4d79-856d-2cb6ece0b3a0",
   "metadata": {},
   "source": [
    "**Exercise**: Find the 2.5th and 97.5th quantiles of the sampling distribution.\n",
    "\n",
    "- These values capture approximately 95% of the point estimates of our neighbors.\n",
    "\n",
    "- These are known as 95% confidence intervals constructed via the percentile method.\n",
    "\n",
    "- `np.quantile(x, [0.025, 0.975])` calculates the 2.5th and 97.5th percentiles of `x`."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "0c220bea-c7a7-4d30-8f6a-6a5f5dfa91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa9b1f9-a13d-4055-8ad3-6a0a23714ab5",
   "metadata": {},
   "source": [
    "#### \ud83d\udd14 The central limit theorem"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e95e2b08-ab58-435e-aa6d-47b6d2a7de7b",
   "metadata": {},
   "source": [
    "Here's where things get spooky.\n",
    "\n",
    "Coarsely, if an estimator involves a summation of random variables, and we sample a sufficient number of data points, then the sampling distribution of the estimator will approximate a normal distribution.\n",
    "\n",
    "The sample proportion of blue M&M's (our *estimator*) has the following formula:\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i = 1}^{N}X_i$$\n",
    "\n",
    "$N$ is the number of M&M's in the bag, $x=1$ denotes a blue M&M, and $x=0$ denotes a not blue M&M.\n",
    "\n",
    "Our *estimator* for the population proportion of M&M's that are blue is calculated by *summing up* all of the observed M&M's that are blue. If we also assume that the color of each M&M is randomly generated, then the central limit theorem is satisfied."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1075f60e-bfd3-4d97-b975-5f502922cf2f",
   "metadata": {},
   "source": [
    "**Exercise**: Assume that the sampling distribution of the population proportion of M&M's that are blue is approximately normal.\n",
    "\n",
    "- Calculate the standard deviation of the sampling distribution. This is known as the *standard error*.\n",
    "\n",
    "- `np.std` may come in handy for calculating the standard error.\n",
    "\n",
    "- Approximately 95% of the mass of a normal distribution falls between 1.96 standard deviations below the mean and 1.96 standard deviations above the mean. Calculate these bounds for the sampling distribution.\n",
    "\n",
    "- This is known as a *95% confidence interval* constructed via the normal approximation.\n",
    "\n",
    "- `np.mean` may also come in handy."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "923d479e-0a24-49fb-bb9f-00aff562f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe30d4-6bcb-4156-8a80-a932317bc8da",
   "metadata": {},
   "source": [
    "## \ud83e\udd7e Constructing parallel universes via the bootstrap"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6994b4d9-7eea-46b7-a564-8685d3d5fe20",
   "metadata": {},
   "source": [
    "As mentioned above, we typically do not observe parallel universes. So, what can we do to estimate the sampling distribution?\n",
    "\n",
    "*The bootstrap* is a tool for constructing parallel universes using a single sample of data.\n",
    "\n",
    "Here are the steps to the bootstrap method:\n",
    "\n",
    "0. Collect a sample of data of size `n`.\n",
    "\n",
    "1. Construct a synthetic sample of size `n` by resampling your data *with replacement*. In other words, after you draw a value from your real sample, *put it back* before drawing another value. \n",
    "\n",
    "2. With your resampled data set, calculate a synthetic point estimate.\n",
    "\n",
    "3. Repeat steps 1 and 2 many times. \n",
    "\n",
    "4. Use the distribution of synthetic point estimates to approximate the sampling distribution of your estimator.\n",
    "\n",
    "For example, suppose you want to estimate the fraction of days each year with rain. Here's how to generate a synthetic sampling distribution via the bootstrap, using a single year's worth of data:"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "35c29122-9d34-41f3-bf45-5eaca2a534f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of data for one year (365 days)\n",
    "rainy_days = 20\n",
    "prob_rain = rainy_days / 365\n",
    "print(prob_rain)\n",
    "\n",
    "# generate 100 synthetic years of data (and 100 synthetic point estimates)\n",
    "synth_rainy_days = np.random.binomial(n=365, p=prob_rain, size=100) / 365\n",
    "\n",
    "sns.histplot(synth_rainy_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c77169-57bc-4e54-b0df-eba8f3d012a9",
   "metadata": {},
   "source": [
    "Here's how to get a 95% confidence interval via the percentile method and the normal approximation:"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "1c6c888a-f06a-4c9d-87eb-3399a17d0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile method\n",
    "print(np.quantile(synth_rainy_days, [0.025, 0.975]))\n",
    "\n",
    "# Normal approximation\n",
    "se = np.std(synth_rainy_days)\n",
    "print([prob_rain-2*se, prob_rain+2*se])\n",
    "\n",
    "# Note that we use the real data for our \n",
    "# point estimate in the normal approximation.\n",
    "# We only use the synthetic sampling distribution to estimate\n",
    "# the standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce3a2b-e3ac-412d-96f8-8e3d773119f1",
   "metadata": {},
   "source": [
    "In this case, both intervals are similar.\n",
    "\n",
    "> Note that the bootstrap can be used for *any* estimator, not just those whose sampling distributions follow an approximate normal distribution. For example, you could use the bootstrap to estimate the Xth percentile of a data distribution."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "74c38bae-5027-497a-be94-57701b2f0ee4",
   "metadata": {},
   "source": [
    "**Exercise**: Using your bag of M&M's, do the following:\n",
    "\n",
    "1. Randomly generate 1,000 synthetic bags of M&M's using the number of M&M's in your bag and the fraction of M&M's in your bag that are blue.\n",
    "\n",
    "2. For each synthetic bag, calculate the fraction of M&M's that are blue.\n",
    "\n",
    "3. Find the 2.5th and 97.5th quantiles of your synthetic sampling distribution. \n",
    "\n",
    "4. Use your synthetic sampling distribution to estimate the standard error of the sample proportion of M&M's that are blue. Using the synthetic standard error and the corresponding point estimate from the real data, construct a 95% confidence interval via the normal approximation.\n",
    "\n",
    "5. How do the constructed intervals compare to each other, and to the class-wide interval?"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "a7e7d409-8b9a-4a22-b06f-43b01989d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3240151-989f-42ce-9d1e-4bc6b48b029d",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Constructing parallel universes via statistics"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7a6ffaef-be46-4be7-98ec-4d2b368526f3",
   "metadata": {},
   "source": [
    "We can also compute the properties of our estimator $\\hat{p}$ analytically,\n",
    "without simulation. It's time for notation!\n",
    "\n",
    "$p$: the *population* proportion of M&M's that are blue\n",
    "\n",
    "$\\hat{p}$: the *sample* proportion of M&M's that are blue\n",
    "\n",
    "> When you see a $\\hat{hat}$ on a variable, it usually means it's an estimate for the same variable without the hat.\n",
    "\n",
    "Let's also assume that an M&M's color is a random variable $X$, where each $X_i$ is generated i.i.d. (independently and identically) via a Bernoulli distribution with probability of success $p$. In other words,\n",
    "\n",
    "$$ X \\sim Bernoulli(p) $$\n",
    "\n",
    "$x=1$ denotes a blue M&M, and $x=0$ denotes a not blue M&M.\n",
    "\n",
    "As stated above, the sample proportion of blue M&M's $\\hat{p}$ has the following formula:\n",
    "\n",
    "$$\\hat{p} = \\frac{1}{N}\\sum_{i = 1}^{N}X_i$$\n",
    "\n",
    "From the [properties of a Bernoulli\n",
    "distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) we know that\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_p(X_i) & = p \\\\\n",
    "\\mathrm{Var}_p(X_i) & = p(1-p) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where the subscript $_p$ in \n",
    "$\\mathbb{E}_p$ and $\\mathrm{Var}_p$ simply means that $p$ is fixed (i.e., it's _not_ random). \n",
    "\n",
    "> Remember, though, even though $p$ is fixed, it's unknown. Estimating $p$, the population proportion of M&M's that are blue, is the whole point of our inference! "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "57bb2e81-8c95-4e25-bfe1-a59df642135a",
   "metadata": {},
   "source": [
    "### \ud83d\udcad The theoretical sampling distribution of $\\hat{p}$"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "30fd8c16-67c4-4d23-9a44-4ef88a3b114e",
   "metadata": {},
   "source": [
    "Suppose we use $\\hat{p}$ to estimate $p$, as we've done above. \n",
    "\n",
    "1. What's the expected value of $\\hat{p}$?\n",
    "\n",
    "2. What's the variance of $\\hat{p}$? Note that the variance of $\\hat{p}$ is the square of its standard error."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e6c1c4b0-690c-4285-bfa5-9ae6f32cebae",
   "metadata": {},
   "source": [
    "Let's turn the statistical crank!\n",
    "\n",
    "#### The expected value of $\\hat{p}$\n",
    "\n",
    "**Linearity of expectation** implies the following:\n",
    "\n",
    "$$\\mathbb{E} \\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n \\mathbb{E}(X_i)$$"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ea7640-f30a-4967-bdf8-054f59e14560",
   "metadata": {},
   "source": [
    "Thus,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_p(\\hat{p}) & = \\mathbb{E}_p\\left(\\frac{1}{N}\\sum_{i = 1}^{N}X_i\\right) \\\\\n",
    "  & = \\frac{1}{N}\\mathbb{E}_p\\left(\\sum_{i = 1}^{N}X_i\\right) \\\\\n",
    "  & = \\frac{1}{N}\\sum_{i = 1}^{N}\\mathbb{E}_p(X_i) \\\\\n",
    "  & = \\frac{1}{N}\\sum_{i = 1}^{N}p \\\\\n",
    "  & = \\frac{1}{N}Np \\\\\n",
    "  & = p\n",
    "\\end{align*}"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3e41a79b-3b52-4163-a315-ea30195ffbec",
   "metadata": {},
   "source": [
    "The expected value of $\\hat{p}$ is $p$.\n",
    "\n",
    "$\\hat{p}$ is therefore an *unbiased* estimator of $p$."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0b00d213-2d7d-4d0b-beaf-1c71c342bd81",
   "metadata": {},
   "source": [
    "#### The variance of $\\hat{p}$"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "57992349-4b53-4eb9-a914-dfceb5607a9d",
   "metadata": {},
   "source": [
    "If each $X_i$ is independently generated, then the following is true:\n",
    "\n",
    "$$\\mathrm{Var} \\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n \\mathrm{Var}(X_i)$$"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "67195497-e7e6-492d-90f7-0046026df2ba",
   "metadata": {},
   "source": [
    "Thus,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{Var}_p(\\hat{p})\n",
    "  & = \\mathrm{Var}_p\\left(\\frac{1}{N}\\sum_{i = 1}^{N}X_i\\right) \\\\\n",
    "  & = \\frac{1}{N^2}\\sum_{i = 1}^{N}\\mathrm{Var}_p(X_i) \\\\\n",
    "  & = \\frac{1}{N^2}Np(1-p) \\\\\n",
    "  & = \\frac{p(1-p)}{N}\n",
    "\\end{align*}\n",
    "\n",
    "and the standard error is:\n",
    "\n",
    "$$\\sqrt{\\mathrm{Var}_p(\\hat{p})} = \\sqrt{\\frac{p(1-p)}{N}}$$"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ef34ed69-c259-40b3-9fe9-e2a872f346b2",
   "metadata": {},
   "source": [
    "If we don't know $p$, how can we calculate the variance of $\\hat{p}$? \n",
    "\n",
    "We have to estimate $p$ using our point estimate, $\\hat{p}$.\n",
    "\n",
    "The estimated variance of $\\hat{p}$ is $\\frac{\\hat{p}(1-\\hat{p})}{n}$, and the estimated standard error is $\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$."
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a9aa2767-979c-43a9-94c4-2be29600882f",
   "metadata": {},
   "source": [
    "### \ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 Putting it all together"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4e999720-bc3c-4ec6-acb5-9ecdb85e8e20",
   "metadata": {},
   "source": [
    "**Exercise**: Using your M&M's data and the formula for the estimated standard error of $\\hat{p}$, use the normal approximation to construct a 95% confidence interval for $p$. \n",
    "\n",
    "- This is the 95% theoretical normal confidence interval for $p$.\n",
    "\n",
    "- `np.sqrt` may come in handy\n",
    "\n",
    "- How does the interval compare to the intervals calculated above?"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "b480871e-767a-4292-84fc-4b18fce9a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}